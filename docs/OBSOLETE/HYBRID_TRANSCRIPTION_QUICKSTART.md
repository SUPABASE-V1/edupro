# Hybrid Transcription Quick Start Guide

**Goal:** Real-time transcription feel using batch method + ASR  
**Time to First Implementation:** 2-3 hours (Phase 1 basics)

---

## üöÄ Phase 1: Quick Wins (Start Here)

### Step 1: Audio Compression (15 minutes)

**File:** `lib/voice-pipeline.ts`

```typescript
// Find DEFAULT_CONFIG and update:
const DEFAULT_CONFIG: VoiceRecordingConfig = {
  quality: 'adaptive',
  maxDuration: 300000,
  silenceThreshold: 0.05,
  silenceDetectionMs: 2000,
  enableBackgroundRecording: false,
  enableNoiseSupression: true,
  enableAutoGainControl: true,
  chunkSizeMs: 1000,  // NEW: 1 second chunks
  compressionBitrate: 32000,  // CHANGED: from 64000 to 32000
  transport: 'auto',
  enableLiveTranscription: true,
};

// Update getAudioConfig function:
export function getAudioConfig(quality: AudioQuality): Audio.RecordingOptions {
  // ... existing code ...
  
  const configs: Record<Exclude<AudioQuality, 'adaptive'>, Audio.RecordingOptions> = {
    low: {
      isMeteringEnabled: true,
      android: {
        extension: '.m4a',
        outputFormat: Audio.AndroidOutputFormat.MPEG_4,
        audioEncoder: Audio.AndroidAudioEncoder.AAC,
        sampleRate: 16000,  // CHANGED: from 22050
        numberOfChannels: 1,
        bitRate: 32000,
      },
      // ... similar changes for iOS and web
    },
    // ... update medium and high similarly
  };
}
```

**Result:** 84% smaller files, faster uploads

---

### Step 2: Create Edge Function Endpoints (2 hours)

**Location:** `supabase/functions/ai-proxy-asr/`

#### 2a. Create `transcribe-chunk/index.ts`

```bash
mkdir -p supabase/functions/ai-proxy-asr/transcribe-chunk
```

```typescript
import { serve } from "https://deno.land/std@0.168.0/http/server.ts"

const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY")!
const OPENAI_MODEL = Deno.env.get("OPENAI_TRANSCRIPTION_MODEL") || "whisper-1"

serve(async (req: Request) => {
  if (req.method !== 'POST') {
    return new Response('Method Not Allowed', { status: 405 })
  }

  try {
    // Auth check
    const authHeader = req.headers.get('Authorization')
    if (!authHeader) {
      return new Response('Unauthorized', { status: 401 })
    }

    // Parse multipart form data
    const formData = await req.formData()
    const audioFile = formData.get('audio') as File
    const sessionId = formData.get('session_id') as string
    const chunkIndex = parseInt(formData.get('chunk_index') as string)
    
    if (!audioFile || !sessionId || chunkIndex === undefined) {
      return new Response('Missing required fields', { status: 400 })
    }

    // Check size limit (2MB)
    if (audioFile.size > 2 * 1024 * 1024) {
      return new Response('Chunk too large', { status: 413 })
    }

    // Send to Whisper
    const whisperForm = new FormData()
    whisperForm.append('file', audioFile)
    whisperForm.append('model', OPENAI_MODEL)
    whisperForm.append('language', 'en')
    whisperForm.append('response_format', 'json')

    const startTime = Date.now()
    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
      },
      body: whisperForm,
    })

    if (!response.ok) {
      throw new Error(`Whisper API error: ${response.status}`)
    }

    const result = await response.json()
    const latency = Date.now() - startTime

    return new Response(JSON.stringify({
      transcript: result.text || '',
      chunk_index: chunkIndex,
      session_id: sessionId,
      language: 'en',
      latency_ms: latency,
    }), {
      status: 200,
      headers: { 'Content-Type': 'application/json' }
    })

  } catch (error: any) {
    console.error('Chunk transcription error:', error)
    return new Response(JSON.stringify({ 
      error: 'Transcription failed',
      details: error.message 
    }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' }
    })
  }
})
```

#### 2b. Create `transcribe-final/index.ts`

```typescript
// Similar structure to transcribe-chunk, but:
// - No chunk_index
// - Can handle larger files (up to 25MB)
// - Returns full metadata (confidence, duration, etc.)
```

#### 2c. Deploy Functions

```bash
supabase functions deploy ai-proxy-asr/transcribe-chunk
supabase functions deploy ai-proxy-asr/transcribe-final
```

---

### Step 3: Web Chunking Client (1-2 hours)

**File:** `lib/speech/chunked-transcription.ts` (NEW)

```typescript
import { assertSupabase } from '@/lib/supabase';
import { v4 as uuidv4 } from 'uuid';

export interface ChunkTranscriptResult {
  text: string;
  chunkIndex: number;
  latency: number;
}

export class ChunkedTranscription {
  private sessionId: string;
  private chunkIndex = 0;
  private chunkMap = new Map<number, string>();
  private mediaRecorder: MediaRecorder | null = null;
  
  constructor() {
    this.sessionId = uuidv4();
  }
  
  async startChunking(
    stream: MediaStream,
    onChunkResult: (result: ChunkTranscriptResult) => void
  ) {
    if (typeof MediaRecorder === 'undefined') {
      throw new Error('MediaRecorder not supported');
    }
    
    // Use Opus codec if available
    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
      ? 'audio/webm;codecs=opus'
      : 'audio/webm';
    
    this.mediaRecorder = new MediaRecorder(stream, {
      mimeType,
      audioBitsPerSecond: 32000,
    });
    
    // Capture chunks every 1 second
    this.mediaRecorder.ondataavailable = async (event) => {
      if (event.data && event.data.size > 0) {
        const chunkIdx = this.chunkIndex++;
        await this.uploadChunk(event.data, chunkIdx, onChunkResult);
      }
    };
    
    this.mediaRecorder.start(1000); // 1 second timeslice
  }
  
  private async uploadChunk(
    blob: Blob,
    chunkIdx: number,
    onResult: (result: ChunkTranscriptResult) => void
  ) {
    try {
      const supabase = assertSupabase();
      const formData = new FormData();
      formData.append('audio', blob, `chunk-${chunkIdx}.webm`);
      formData.append('session_id', this.sessionId);
      formData.append('chunk_index', chunkIdx.toString());
      
      const { data, error } = await supabase.functions.invoke(
        'ai-proxy-asr/transcribe-chunk',
        {
          body: formData,
        }
      );
      
      if (error) throw error;
      
      if (data?.transcript) {
        this.chunkMap.set(chunkIdx, data.transcript);
        onResult({
          text: data.transcript,
          chunkIndex: chunkIdx,
          latency: data.latency_ms || 0,
        });
      }
    } catch (error) {
      console.error(`Chunk ${chunkIdx} failed:`, error);
      // Continue with other chunks
    }
  }
  
  stop() {
    if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
      this.mediaRecorder.stop();
    }
  }
  
  getStitchedTranscript(): string {
    const indices = Array.from(this.chunkMap.keys()).sort((a, b) => a - b);
    return indices.map(idx => this.chunkMap.get(idx)).join(' ');
  }
}
```

---

### Step 4: Test It!

**File:** `test-chunked-transcription.ts` (NEW)

```typescript
import { ChunkedTranscription } from './lib/speech/chunked-transcription';

async function testChunking() {
  console.log('üé§ Starting chunked transcription test...');
  
  // Get microphone access
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  
  const chunker = new ChunkedTranscription();
  
  let fullText = '';
  
  await chunker.startChunking(stream, (result) => {
    console.log(`üìù Chunk ${result.chunkIndex} (${result.latency}ms):`, result.text);
    fullText += result.text + ' ';
    console.log('Full text so far:', fullText.trim());
  });
  
  // Record for 10 seconds
  setTimeout(() => {
    chunker.stop();
    stream.getTracks().forEach(track => track.stop());
    console.log('‚úÖ Final transcript:', chunker.getStitchedTranscript());
  }, 10000);
}

// Run test
testChunking().catch(console.error);
```

**Run:**
```bash
npm run web
# Open browser console
# Paste the test code
```

---

## üìä Expected Results

### Before (Batch Only)
- User speaks for 10 seconds
- Waits 5-10 seconds after stopping
- Finally sees transcript

### After (Chunked Batch)
- User speaks for 10 seconds
- Sees first words after ~1.5 seconds
- Sees progressive updates every 1-2 seconds
- Final result ready when they stop

---

## üîß Environment Variables

Add to `.env`:

```bash
# Feature flags
EXPO_PUBLIC_ENABLE_CHUNKED_TRANSCRIPTION=true
EXPO_PUBLIC_CHUNK_DURATION_MS=1000

# Backend (set in Supabase Dashboard ‚Üí Edge Functions ‚Üí Secrets)
OPENAI_API_KEY=sk-...
OPENAI_TRANSCRIPTION_MODEL=whisper-1
```

---

## üêõ Troubleshooting

### "MediaRecorder not supported"
- Use Chrome/Edge (Safari support limited)
- Check HTTPS (required for getUserMedia)

### "Unauthorized" from Edge Function
- Verify Supabase JWT in Authorization header
- Check function deployment: `supabase functions list`

### Chunks arrive out of order
- Use chunk_index to reorder
- Final pass will fix this

### Audio quality poor
- Check bitrate settings (should be 32kbps)
- Verify sample rate (16kHz)
- Test microphone input levels

---

## üìà Performance Monitoring

Add to your analytics:

```typescript
analytics.track('chunked_transcription_session', {
  session_id: sessionId,
  total_chunks: chunkCount,
  avg_chunk_latency_ms: avgLatency,
  total_duration_ms: totalDuration,
  platform: 'web',
});
```

---

## üéØ Next Steps

1. ‚úÖ Get Phase 1 working (chunked transcription on web)
2. Add on-device ASR (Tier 1) for instant feedback
3. Add final pass (Tier 3) for accuracy
4. Implement hybrid manager to coordinate all tiers
5. Port to mobile (Android/iOS)

---

## üìö Related Docs

- [HYBRID_REALTIME_TRANSCRIPTION_PLAN.md](./docs/voice/HYBRID_REALTIME_TRANSCRIPTION_PLAN.md) - Full architecture
- [VOICE_TRANSCRIPTION_ENHANCEMENTS.md](./docs/VOICE_TRANSCRIPTION_ENHANCEMENTS.md) - Existing batch system
- [TRANSCRIPTION_SPEED_OPTIMIZATION.md](./docs/performance/TRANSCRIPTION_SPEED_OPTIMIZATION.md) - Performance analysis

---

**Time Investment:**
- Phase 1 (this guide): 2-3 hours
- Phase 2 (full hybrid): +8-12 hours
- Phase 3 (WebRTC optional): +10-15 hours

**Impact:**
- 90-95% reduction in perceived latency
- 10x faster user experience
- Same accuracy as current system

üöÄ **Start coding!**
