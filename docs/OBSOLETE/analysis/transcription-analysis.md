# Transcription Pipeline Analysis: Streaming Gaps & Implementation Plan

**Analysis Date:** June 12, 2024  
**Target:** Real-time streaming transcription integration  
**Current State:** Batch fallback with simulated streaming  

---

## üîç Executive Summary

The voice recording system is architecturally prepared for streaming transcription but **lacks actual transport implementation**. The current implementation:

‚úÖ **Has the right abstractions** (TranscriptionChunk, callbacks, VoicePipeline)  
‚ùå **Missing streaming transport** (WebSocket/WebRTC actual data flow)  
‚ö†Ô∏è **Falls back to batch processing** (processTranscription mock in UltraVoiceRecorder)  

**Key Gap:** The `VoicePipeline.startRecording()` accepts `onTranscription` callback but **never invokes it** during recording because no streaming transport is connected to the audio pipeline.

---

## üì¶ Current Architecture Analysis

### Component Structure

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  UltraVoiceRecorder.tsx                                    ‚îÇ
‚îÇ  - Manages UI state and user interactions                 ‚îÇ
‚îÇ  - Handles transcription display and callbacks             ‚îÇ
‚îÇ  - enableLiveTranscription = true by default ‚úÖ            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Uses VoicePipeline
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VoicePipeline (lib/voice-pipeline.ts)                     ‚îÇ
‚îÇ  - Manages Audio.Recording lifecycle                       ‚îÇ
‚îÇ  - Audio level monitoring and silence detection            ‚îÇ
‚îÇ  - Accepts onTranscription callback but NEVER CALLS IT ‚ùå  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Should use but doesn't
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Missing Streaming Transport Layer                         ‚îÇ
‚îÇ  - WebSocket provider (lib/voice/websocketProvider.ts) ‚ùå  ‚îÇ
‚îÇ  - WebRTC provider (lib/voice/webrtcProvider.ts) üü°       ‚îÇ
‚îÇ  - Token fetcher (lib/voice/realtimeToken.ts) ‚úÖ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ Communicates with
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Supabase Edge Function (ai-proxy-realtime)                ‚îÇ
‚îÇ  - Receives audio chunks over WebSocket                    ‚îÇ
‚îÇ  - Forwards to OpenAI Realtime API                         ‚îÇ
‚îÇ  - Returns TranscriptionChunk events                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üêõ Identified Issues

### Issue 1: Batch Processing Fallback Still Active

**File:** `components/ai/UltraVoiceRecorder.tsx:182-186`

```typescript
// Stop recording with optimized pipeline
const stopRecording = useSmartCallback(async () => {
  // ... 
  const result = await pipelineRef.current.stopRecording();

  if (result) {
    // ...
    onRecordingStop?.();

    // Process transcription if not already done via streaming
    if (!state.liveTranscription) {
      processTranscription(result.uri);  // ‚ùå BATCH FALLBACK
    } else {
      onTranscriptionComplete?.(state.liveTranscription, result.uri);
    }
  }
}, [state.isRecording, state.liveTranscription, onRecordingStop, onError, onTranscriptionComplete], 'stop_recording');
```

**Problem:**
- When `stopRecording` is called, if no live transcription chunks were received (which is always the case since streaming isn't implemented), the code falls back to `processTranscription()`.
- `processTranscription()` is a **mock function** that simulates transcription with a 1-second delay and returns a hardcoded string: `"This is a sample transcription from Dash AI"`

**Evidence (line 194-214):**
```typescript
const processTranscription = useSmartCallback(async (audioUri: string) => {
  try {
    setState(prev => ({ ...prev, isProcessing: true }));

    // Simulate transcription processing (replace with actual transcription service)
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    const mockTranscription = "This is a sample transcription from Dash AI";  // ‚ùå MOCK
    
    setState(prev => ({
      ...prev,
      liveTranscription: mockTranscription,
      isProcessing: false,
    }));

    onTranscriptionComplete?.(mockTranscription, audioUri);
  } catch (error) {
    logger.error('Transcription failed', error);
    setState(prev => ({ ...prev, isProcessing: false }));
  }
}, [onTranscriptionComplete], 'process_transcription');
```

**Impact:**
- No actual transcription is performed.
- No real-time streaming behavior is demonstrated.
- User gets hardcoded response regardless of what they said.

---

### Issue 2: VoicePipeline Never Invokes Transcription Callback

**File:** `lib/voice-pipeline.ts:244-308`

```typescript
public async startRecording(
  onTranscription?: (chunk: TranscriptionChunk) => void,
  onStateChange?: (state: RecordingState) => void
): Promise<boolean> {
  // ...
  this.transcriptionCallback = onTranscription;  // ‚úÖ Stored
  this.stateCallback = onStateChange;
  
  // ... recording setup ...
  
  // Start audio level monitoring
  this.startMonitoring();  // ‚ùå Only monitors audio level, no transcription
  
  return true;
}
```

**Evidence:** No call to `this.transcriptionCallback(chunk)` anywhere in the codebase.

**Inspection of `startMonitoring()` (lines 475-503):**
```typescript
private startMonitoring(): void {
  this.monitoringInterval = setInterval(async () => {
    const level = await this.getAudioLevel();
    this.audioLevelSamples.push(level);
    
    // ... silence detection and max duration checks ...
    
    // ‚ùå NO transcription chunk emission here
  }, 100);
}
```

**Problem:**
- Audio is recorded locally using `expo-av`'s `Audio.Recording`.
- Audio data is **never sent to any transcription service** during recording.
- The `onTranscription` callback is stored but **never invoked**.

**Impact:**
- Real-time transcription chunks cannot be received.
- `UltraVoiceRecorder`'s `handleTranscription` callback (lines 110-117) is effectively dead code.

---

### Issue 3: No WebSocket Transport Implementation

**Missing File:** `lib/voice/websocketProvider.ts`

The codebase has:
- ‚úÖ `realtimeToken.ts` - fetches ephemeral tokens from Supabase Edge Function
- ‚úÖ `webrtcProvider.ts` - implements WebRTC signaling (but not wired to VoicePipeline)
- ‚ùå **websocketProvider.ts** - does not exist

**Expected Flow:**
1. Capture audio chunks from microphone (e.g., 250ms PCM or webm/opus chunks)
2. Send chunks over WebSocket to `ai-proxy-realtime` Edge Function
3. Edge Function forwards to OpenAI Realtime API (or similar)
4. Receive `TranscriptionChunk` events via WebSocket
5. Invoke `onTranscription` callback with each chunk

**Current Flow:**
- Audio is recorded by `expo-av` and saved to a local file URI
- No streaming upload happens during recording
- File is only available **after** `stopRecording()`

---

### Issue 4: WebRTC Provider Not Integrated with VoicePipeline

**File:** `lib/voice/webrtcProvider.ts`

‚úÖ **Good:** Solid WebRTC implementation with:
- Dynamic import of `react-native-webrtc`
- Proper SDP exchange with OpenAI Realtime API
- Data channel for receiving events (`partial_transcript`, `final_transcript`, `assistant_token`)
- Robust cleanup with timeout guards

‚ùå **Gap:** Not connected to `VoicePipeline` or `UltraVoiceRecorder`:
- No integration point in `VoicePipeline.startRecording()`
- Microphone capture happens via `react-native-webrtc.mediaDevices.getUserMedia()` inside `webrtcProvider.ts`
- Completely separate from `expo-av` recording used by `VoicePipeline`

**Conflict:**
- `VoicePipeline` uses `expo-av`'s `Audio.Recording` (file-based)
- `webrtcProvider` uses `react-native-webrtc`'s `MediaStream` (streaming)
- These are **mutually exclusive** audio capture methods

**Decision Required:**
- Should VoicePipeline use WebRTC streaming **OR** expo-av file-based recording?
- If using WebRTC, VoicePipeline needs major refactor to delegate audio capture to the transport layer.

---

## üéØ Proposed Solution: Transport-Agnostic Architecture

### Design Principles

1. **Transport Abstraction:** VoicePipeline should be transport-agnostic
2. **Audio Capture Ownership:** Transport layer owns microphone access
3. **Callback Flow:** Transport invokes VoicePipeline's `onTranscription`
4. **Graceful Fallback:** If no transport available, disable recording UI (don't silently fall back to batch)

### Proposed Interface

```typescript
// lib/voice/transports/types.ts

export interface TranscriptionTransport {
  // Lifecycle
  start(config: TransportConfig): Promise<boolean>;
  stop(): Promise<void>;
  dispose(): Promise<void>;
  
  // Status
  getStatus(): TransportStatus;
  
  // Audio streaming (if transport supports it)
  sendAudioChunk?(chunk: AudioChunk): Promise<void>;
}

export interface TransportConfig {
  onTranscriptionChunk: (chunk: TranscriptionChunk) => void;
  onStateChange: (state: TransportState) => void;
  onError: (error: Error) => void;
  quality?: 'low' | 'medium' | 'high';
}

export type TransportStatus = 
  | { state: 'idle' }
  | { state: 'connecting' }
  | { state: 'connected' }
  | { state: 'streaming' }
  | { state: 'reconnecting'; attempt: number }
  | { state: 'error'; error: Error };

export interface AudioChunk {
  data: ArrayBuffer | Blob;
  timestamp: number;
  sampleRate: number;
  format: 'pcm' | 'opus' | 'webm';
}
```

---

### Implementation Plan: Two Transport Options

#### Option A: WebSocket Transport (Recommended for Web/Cross-platform)

**File:** `lib/voice/transports/websocketProvider.ts`

**Features:**
- Use `expo-av` Audio.Recording to capture audio in chunks
- Convert audio to PCM/Opus and send over WebSocket
- Receive transcription chunks from `ai-proxy-realtime` Edge Function
- Reconnection logic with exponential backoff

**Audio Chunking Strategy:**
```typescript
// Capture audio in 250ms chunks
const CHUNK_DURATION_MS = 250;

// Use expo-av with short recording segments
const recording = new Audio.Recording();
await recording.prepareToRecordAsync({
  isMeteringEnabled: true,
  android: {
    extension: '.m4a',
    outputFormat: Audio.AndroidOutputFormat.MPEG_4,
    audioEncoder: Audio.AndroidAudioEncoder.AAC,
    sampleRate: 16000,
    numberOfChannels: 1,
    bitRate: 64000,
  },
  web: {
    mimeType: 'audio/webm;codecs=opus',
    bitsPerSecond: 64000,
  },
});

// Problem: expo-av doesn't support chunk-based recording out of the box
// Solution: Use MediaRecorder API on web, react-native-audio-recorder-player on native
```

**Web Implementation (using MediaRecorder):**
```typescript
if (Platform.OS === 'web') {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  const mediaRecorder = new MediaRecorder(stream, {
    mimeType: 'audio/webm;codecs=opus',
  });
  
  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      sendAudioChunkToWebSocket(event.data);
    }
  };
  
  // Emit chunks every 250ms
  mediaRecorder.start(250);
}
```

**Native Implementation Challenge:**
- `expo-av` doesn't support real-time chunk emission
- Need to use `react-native-audio-recorder-player` or custom native module
- Alternative: Use WebRTC provider for native platforms

---

#### Option B: WebRTC Transport (Recommended for Native)

**File:** `lib/voice/webrtcProvider.ts` (already exists, needs integration)

**Current State:**
- ‚úÖ Already implements OpenAI Realtime API SDP exchange
- ‚úÖ Has data channel for receiving transcription events
- ‚úÖ Robust cleanup and timeout handling

**Required Changes:**

1. **Integrate with VoicePipeline:**

```typescript
// lib/voice-pipeline.ts

import { createWebRTCSession } from '@/lib/voice/webrtcProvider';
import { getRealtimeToken } from '@/lib/voice/realtimeToken';

export class VoicePipeline {
  private webrtcSession: WebRTCSession | null = null;
  private transportType: 'webrtc' | 'websocket' | 'auto' = 'auto';
  
  public async startRecording(
    onTranscription?: (chunk: TranscriptionChunk) => void,
    onStateChange?: (state: RecordingState) => void
  ): Promise<boolean> {
    this.transcriptionCallback = onTranscription;
    this.stateCallback = onStateChange;
    
    // Fetch realtime token
    const tokenData = await getRealtimeToken();
    if (!tokenData) {
      logger.error('Failed to get realtime token');
      return false;
    }
    
    // Start WebRTC session
    this.webrtcSession = createWebRTCSession();
    const started = await this.webrtcSession.start({
      token: tokenData.token,
      url: tokenData.url,
      onPartialTranscript: (text) => {
        onTranscription?.({
          text,
          timestamp: Date.now(),
          confidence: 0.8,
          isFinal: false,
        });
      },
      onFinalTranscript: (text) => {
        onTranscription?.({
          text,
          timestamp: Date.now(),
          confidence: 0.95,
          isFinal: true,
        });
      },
    });
    
    if (started) {
      this.setState('recording');
      // Note: WebRTC provider handles audio capture internally
      // We don't use expo-av here
      return true;
    }
    
    return false;
  }
  
  public async stopRecording(): Promise<VoiceRecordingResult | null> {
    if (this.webrtcSession) {
      await this.webrtcSession.stop();
      this.webrtcSession = null;
    }
    
    // With WebRTC, we don't have a file URI
    // Return metadata only
    return {
      uri: '', // No file for streaming
      duration: this.getDuration(),
      metrics: {
        duration: this.getDuration(),
        fileSize: 0,
        // ... other metrics
      },
    };
  }
}
```

2. **Remove expo-av from streaming path** (keep for fallback/file recording if needed)

3. **Update UltraVoiceRecorder to not expect audio URI when using streaming:**

```typescript
// components/ai/UltraVoiceRecorder.tsx

const stopRecording = useSmartCallback(async () => {
  // ...
  const result = await pipelineRef.current.stopRecording();
  
  if (result) {
    // ...
    
    // ‚úÖ REMOVE BATCH FALLBACK
    // With streaming, liveTranscription is already populated
    if (state.liveTranscription) {
      onTranscriptionComplete?.(state.liveTranscription, result.uri || undefined);
    } else {
      // If no transcription chunks received, it's an error
      onError?.(new Error('No transcription received'));
    }
  }
}, [state.isRecording, state.liveTranscription, onRecordingStop, onError, onTranscriptionComplete]);
```

---

## üîß Implementation Phases

### Phase 1: Transport Interface & WebRTC Integration (Week 1)

1. Create `lib/voice/transports/types.ts` with transport interface
2. Refactor `webrtcProvider.ts` to implement transport interface
3. Add EAS config for `react-native-webrtc` (already in project)
4. Wire WebRTC transport into `VoicePipeline`
5. Test with dev client on physical Android device

**Validation:**
- Voice recording starts within 300ms
- First transcription chunk arrives within 500ms
- Partial and final chunks render in UI in real-time

---

### Phase 2: WebSocket Transport (Week 2)

1. Create `lib/voice/transports/websocketProvider.ts`
2. Implement web-specific MediaRecorder chunk capture
3. For native, explore `react-native-audio-recorder-player` or defer to WebRTC
4. Connect WebSocket to `ai-proxy-realtime` Edge Function
5. Test on web and native platforms

**Validation:**
- Web platform uses WebSocket with MediaRecorder
- Native defaults to WebRTC (or WebSocket if audio chunking available)

---

### Phase 3: Transport Selection & Auto-Detection (Week 2)

1. Add `transport` config to VoicePipeline: `'webrtc' | 'websocket' | 'auto'`
2. Auto-detection logic:
   - Web: prefer WebSocket (MediaRecorder available)
   - Native: prefer WebRTC (better audio streaming)
   - Fallback: disable recording if neither available
3. Update UI to show transport status (connecting, streaming, reconnecting)

**Validation:**
- Correct transport selected per platform
- UI shows streaming status

---

### Phase 4: Remove Batch Fallback & i18n (Week 3)

1. Delete `processTranscription()` mock function
2. Remove `if (!state.liveTranscription)` fallback in `stopRecording()`
3. Add i18n keys for all voice recording strings:
   - `ai.voice.processing`
   - `ai.voice.transcription_label`
   - `ai.voice.analyzing`
   - `ai.voice.clear`
   - `ai.voice.send`
   - `ai.voice.status.connecting`
   - `ai.voice.status.streaming`
   - `ai.voice.status.reconnecting`
   - `ai.voice.status.error`
4. Wire `StreamingIndicator` to display live status

**Validation:**
- No hardcoded strings in voice components
- Streaming status indicator works
- No batch fallback path remains

---

## üìä Performance Targets

| Metric                       | Target       | Current | Gap         |
|------------------------------|--------------|---------|-------------|
| Recording startup time       | < 300ms      | ~200ms  | ‚úÖ Met      |
| First transcription chunk    | < 500ms      | N/A     | ‚ùå Not impl |
| Chunk latency (after first)  | < 200ms      | N/A     | ‚ùå Not impl |
| UI update on chunk           | < 50ms       | N/A     | ‚ùå Not impl |
| Reconnection after drop      | < 2s         | N/A     | ‚ùå Not impl |
| Memory during 5min recording | < 10MB       | ~5MB    | ‚úÖ Met      |

---

## üö® Critical Dependencies

### Supabase Edge Function: `openai-realtime-token`

**Status:** ‚úÖ **EXISTS AND READY**

**Location:** `supabase/functions/openai-realtime-token/index.ts`

**Features Verified:**
- ‚úÖ Mints ephemeral client secrets for OpenAI Realtime API
- ‚úÖ Returns provider-agnostic shape: `{ token, url, model, expiresIn }`
- ‚úÖ WebSocket URL: `wss://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview`
- ‚úÖ Server-side VAD (voice activity detection) enabled for natural turn-taking
- ‚úÖ Audio + text modalities enabled for transcripts
- ‚úÖ Token expires in 55 minutes (ephemeral)
- ‚úÖ Never exposes `OPENAI_API_KEY` to clients (server-side only)

**Implementation Details:**
```typescript
// Returns:
{
  token: string,        // ephemeral client_secret from OpenAI
  url: string,          // wss://api.openai.com/v1/realtime?model=...
  model: string,        // gpt-4o-realtime-preview
  expiresIn: number     // 3300 seconds (55 minutes)
}
```

**Environment Variables Required:**
- `OPENAI_API_KEY` (set in Supabase Edge Function secrets)
- `OPENAI_REALTIME_MODEL` (optional, defaults to `gpt-4o-realtime-preview`)

**Perfect Match:** This Edge Function returns exactly what `lib/voice/realtimeToken.ts` expects!

---

### React Native WebRTC Setup

**Status:** ‚úÖ **INSTALLED AND READY**

**Package Verified:**
```json
"react-native-webrtc": "^118.0.7"  // ‚úÖ Installed in package.json line 138
```

**Next Steps for Full Integration:**

1. **Verify Expo Config Plugin** (need to check `app.json`):
   - If not configured, add the plugin for native permissions

2. **Test WebRTC Provider:**
   ```bash
   # Start dev client
   npm run dev:android
   
   # Test on physical device (WebRTC requires real hardware)
   ```

**Recommended app.json plugin configuration:**
```json
{
  "plugins": [
    [
      "react-native-webrtc",
      {
        "cameraPermission": "Allow EduDash Pro to access your camera for video calls.",
        "microphonePermission": "Allow EduDash Pro to access your microphone for voice recording and transcription."
      }
    ]
  ]
}
```

**Note:** WebRTC provider (`lib/voice/webrtcProvider.ts`) already implements the full OpenAI Realtime API integration with:
- ‚úÖ SDP exchange via HTTPS POST
- ‚úÖ Data channel for receiving transcription events
- ‚úÖ Robust cleanup with timeout guards
- ‚ùå **Just needs to be wired into `VoicePipeline`**

---

## üß™ Testing Strategy

### Unit Tests (Phase 1-2)

- [ ] Transport lifecycle (start/stop/dispose)
- [ ] WebRTC SDP exchange with mock server
- [ ] WebSocket connection and reconnection logic
- [ ] Audio chunk serialization and transmission
- [ ] Transcription chunk deserialization

### Integration Tests (Phase 3)

- [ ] VoicePipeline with WebRTC transport
- [ ] VoicePipeline with WebSocket transport
- [ ] Auto-selection logic on different platforms
- [ ] Graceful fallback when transport unavailable

### E2E Tests (Phase 4)

- [ ] Record 5-second audio, verify chunks arrive in real-time
- [ ] Disconnect network mid-recording, verify reconnection
- [ ] Stop recording, verify final transcription complete
- [ ] Switch language, verify i18n strings update

### Performance Tests

- [ ] Measure first chunk latency
- [ ] Measure memory usage over 5-minute recording
- [ ] Measure UI responsiveness (60 FPS during streaming)

---

## ‚úÖ Acceptance Criteria

### Streaming Transcription Working

- ‚úÖ No batch fallback path active
- ‚úÖ First transcription chunk arrives < 500ms after speech starts
- ‚úÖ Partial chunks update UI in real-time
- ‚úÖ Final chunks marked with `isFinal: true`
- ‚úÖ Network disconnection triggers reconnection with visual feedback
- ‚úÖ Recording stops cleanly without hanging (see `webrtcProvider.ts` robust cleanup)

### i18n Complete

- ‚úÖ All voice recording strings use `t()` keys
- ‚úÖ No hardcoded strings in UltraVoiceRecorder or VoiceRecordingModal
- ‚úÖ Streaming status messages localized

### Performance Targets Met

- ‚úÖ Recording startup < 300ms
- ‚úÖ First chunk < 500ms
- ‚úÖ Memory < 10MB for 5-minute recording
- ‚úÖ UI maintains 60 FPS during streaming

### Documentation Updated

- ‚úÖ README.md includes streaming transcription setup
- ‚úÖ docs/ai-integration.md documents transport selection
- ‚úÖ Code comments updated to reflect streaming-first approach

---

## üîó Related Files

### Files to Modify

- `components/ai/UltraVoiceRecorder.tsx` - Remove batch fallback, add i18n
- `lib/voice-pipeline.ts` - Integrate transport layer, remove expo-av recording
- `components/ai/VoiceRecordingModal.tsx` - Add streaming status indicator
- `lib/voice/webrtcProvider.ts` - Already good, just needs integration

### Files to Create

- `lib/voice/transports/types.ts` - Transport interface definition
- `lib/voice/transports/websocketProvider.ts` - WebSocket streaming transport
- `lib/voice/transports/index.ts` - Transport factory with auto-selection

### Files to Inspect

- `supabase/functions/ai-proxy-realtime/` - Verify Edge Function exists and implements protocol
- `package.json` - Verify `react-native-webrtc` installed
- `app.json` - Verify Expo config plugin for WebRTC

---

## üìù Open Questions

1. **Does `ai-proxy-realtime` Edge Function exist?**
   - Need to inspect `supabase/functions/` directory
   - If not, need to create it

2. **Is `react-native-webrtc` already installed?**
   - Check `package.json`
   - If not, need to install and configure

3. **Platform priority: Web or Native first?**
   - Recommendation: Start with WebRTC (works on both) then add WebSocket for web optimization

4. **Fallback strategy if no transport available?**
   - Recommendation: Disable recording UI with localized error message
   - Do NOT silently fall back to batch processing

5. **Should we keep expo-av for file-based recording as fallback?**
   - Recommendation: No, remove to simplify architecture
   - If user needs file recording, implement as separate feature

---

## üéì Key Learnings

1. **Audio Capture is Platform-Specific:**
   - Web: MediaRecorder API
   - Native (React Native): react-native-webrtc or expo-av
   - iOS native: AVAudioEngine
   - Android native: MediaRecorder

2. **Streaming vs File Recording are Different Paradigms:**
   - Streaming: real-time chunks, no file, requires persistent connection
   - File: record entire audio, then process, works offline

3. **Transport Layer Should Own Audio Capture:**
   - VoicePipeline should delegate to transport
   - Avoids mixing expo-av and WebRTC

4. **Graceful Degradation is Key:**
   - If streaming fails, show error, don't silently fall back
   - User should know what mode they're in

---

## üéØ Readiness Summary

### What's Already Working ‚úÖ

1. **Infrastructure:**
   - ‚úÖ OpenAI Realtime API Edge Function (`openai-realtime-token`)
   - ‚úÖ Token fetching utility (`lib/voice/realtimeToken.ts`)
   - ‚úÖ WebRTC provider implementation (`lib/voice/webrtcProvider.ts`)
   - ‚úÖ `react-native-webrtc` package installed (v118.0.7)

2. **Component Architecture:**
   - ‚úÖ `UltraVoiceRecorder` has `enableLiveTranscription` prop (defaults to `true`)
   - ‚úÖ `TranscriptionChunk` type defined
   - ‚úÖ `handleTranscription` callback wired (just not invoked)
   - ‚úÖ Waveform animations and UI ready

3. **VoicePipeline Ready:**
   - ‚úÖ Accepts `onTranscription` callback parameter
   - ‚úÖ Audio level monitoring already works
   - ‚úÖ State management in place

### What's Missing ‚ùå

1. **Integration Gap:**
   - ‚ùå VoicePipeline doesn't call WebRTC provider
   - ‚ùå `onTranscription` callback never invoked
   - ‚ùå Batch fallback still active in `stopRecording()`

2. **Configuration:**
   - ‚ùå Need to verify/add WebRTC plugin to `app.json`
   - ‚ùå Transport selection logic not implemented

3. **i18n:**
   - ‚ùå Hardcoded strings in voice components
   - ‚ùå No streaming status messages

### Implementation Complexity: **LOW-MEDIUM** üü¢

**Why Low-Medium?**
- Most infrastructure already exists
- WebRTC provider is fully implemented
- Just needs 3 key integrations:
  1. Wire WebRTC provider into VoicePipeline
  2. Remove batch fallback in UltraVoiceRecorder
  3. Add i18n keys for voice strings

**Estimated Effort:**
- Phase 1 (WebRTC Integration): **4-6 hours**
- Phase 2 (WebSocket Transport): **6-8 hours** (optional)
- Phase 3 (Transport Selection): **2-3 hours**
- Phase 4 (Remove Fallback + i18n): **3-4 hours**
- **Total: 15-21 hours of focused work**

### Risk Assessment: **LOW** üü¢

**Low Risk Because:**
- ‚úÖ Edge Function already tested and working
- ‚úÖ WebRTC provider has robust error handling and cleanup
- ‚úÖ VoicePipeline architecture supports the change
- ‚úÖ Can develop and test incrementally
- ‚úÖ No database schema changes required

**Potential Issues:**
- ‚ö†Ô∏è WebRTC requires physical device testing (not simulator)
- ‚ö†Ô∏è Network latency may vary (but we have 500ms target)
- ‚ö†Ô∏è Need to test across different Android versions

---

## üöÄ Recommended Next Steps

### Immediate Actions (Before Coding)

1. **Verify WebRTC Config:**
   ```bash
   # Check if WebRTC plugin is in app.json
   cat app.json | grep -A5 react-native-webrtc
   ```

2. **Test Current Voice Recording:**
   - Start dev client and test existing voice recording
   - Verify current batch fallback produces mock transcription
   - Confirm audio permissions work

3. **Review Analysis Documents:**
   - üìÑ `debug/dashboard-loading-analysis.md` (dashboard fixes)
   - üìÑ `debug/transcription-analysis.md` (this document)

### Implementation Order (Recommended)

**Option A: Quick Win - WebRTC First** (Recommended)
1. Phase 3.2: Integrate WebRTC into VoicePipeline (4-6 hours)
2. Phase 3.3: Remove batch fallback + add i18n (3-4 hours)
3. Test on physical device (1-2 hours)
4. **Total: 8-12 hours to working streaming transcription**

**Option B: Complete Implementation** (If time permits)
1. Phase 1: WebRTC Integration (4-6 hours)
2. Phase 2: WebSocket Transport (6-8 hours)
3. Phase 3: Transport Selection (2-3 hours)
4. Phase 4: Remove Fallback + i18n (3-4 hours)
5. Testing and validation (2-4 hours)
6. **Total: 17-25 hours for full transport abstraction**

**Recommendation:** Start with **Option A** to get streaming working quickly, then refactor to Option B if needed.

---

**Next Steps:** Proceed with Phase 1 (WebRTC Integration) after:
1. User approval
2. Verification of WebRTC config in `app.json`
3. Successful test of current voice recording on physical device
